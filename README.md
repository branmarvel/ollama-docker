# Ollama con GPU NVIDIA - Docker Setup

Configuraci√≥n completa de Ollama con soporte GPU NVIDIA usando Docker y Docker Compose.

## üìã Requisitos Previos

### Hardware
- GPU NVIDIA con al menos 6GB de VRAM (recomendado 16GB para modelos grandes)
- 8GB de RAM del sistema (m√≠nimo)
- 20GB de espacio en disco para modelos

### Software
- Docker 20.10+
- Docker Compose 2.0+
- NVIDIA Container Toolkit
- NVIDIA Drivers 470+

## üöÄ Instalaci√≥n R√°pida

### 1. NVIDIA Container Toolkit
```bash
# Para Ubuntu/Debian
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

### 2. Verificar Instalaci√≥n
```bash
# Verificar GPU
nvidia-smi

# Verificar Docker con NVIDIA
docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi
```

## üì¶ Uso

### Iniciar Ollama
```bash
# Hacer el script ejecutable
chmod +x scripts/ollama-manager.sh

# Iniciar el servicio
./scripts/ollama-manager.sh up
```

### Descargar Modelos
```bash
# Modelos recomendados para 16GB VRAM:
./scripts/ollama-manager.sh download deepseek-coder:6.7b    # Programaci√≥n
./scripts/ollama-manager.sh download deepseek-llm:7b        # General
./scripts/ollama-manager.sh download codellama:7b           # C√≥digo
./scripts/ollama-manager.sh download mistral:7b             # General
./scripts/ollama-manager.sh download llama2:7b              # General
```

### Probar Ollama
```bash
# Verificar que funciona
./scripts/ollama-manager.sh test

# Ejecutar modelo interactivo
./scripts/ollama-manager.sh exec ollama run deepseek-coder:6.7b
```

## üõ†Ô∏è Configuraci√≥n Personalizada

### Variables de Entorno
Edita el archivo `.env` para personalizar:

```bash
# Modelo a descargar autom√°ticamente
OLLAMA_MODEL=deepseek-coder:6.7b

# Configuraci√≥n del servidor
OLLAMA_HOST=0.0.0.0
OLLAMA_PORT=11434
```

### Modelos Disponibles
Para 16GB VRAM puedes usar:
- **deepseek-coder:6.7b** - Especializado en programaci√≥n
- **deepseek-llm:7b** - Modelo general
- **codellama:7b** - Optimizado para c√≥digo
- **mistral:7b** - Alto rendimiento general
- **llama2:7b** - Modelo general
- **vicuna:7b** - Chat optimizado

## üì° API REST

Ollama expone una API REST en `http://localhost:11434`:

### Ejemplos de uso:
```bash
# Listar modelos
curl http://localhost:11434/api/tags

# Generar respuesta
curl http://localhost:11434/api/generate -d '{
  "model": "deepseek-coder:6.7b",
  "prompt": "¬øCu√°l es la complejidad del quicksort?"
}'

# Chat conversacional
curl http://localhost:11434/api/chat -d '{
  "model": "deepseek-coder:6.7b",
  "messages": [
    {"role": "user", "content": "Expl√≠came la recursi√≥n"}
  ]
}'
```

## üîß Comandos √ötiles

### Gesti√≥n del Contenedor
```bash
# Ver estado
docker-compose ps

# Ver logs
docker-compose logs -f

# Detener
docker-compose down

# Reiniciar
docker-compose restart
```

### Gesti√≥n de Modelos
```bash
# Listar modelos
docker-compose exec ollama ollama list

# Descargar modelo
docker-compose exec ollama ollama pull mistral:7b

# Eliminar modelo
docker-compose exec ollama ollama rm modelo:nombre

# Copiar modelo desde host
docker cp modelo.bin ollama-gpu:/home/ollama/.ollama/models/
```

## üêõ Soluci√≥n de Problemas

### GPU No Detectada
```bash
# Verificar NVIDIA Container Toolkit
nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# Verificar GPU en Docker
docker run --rm --gpus all nvidia/cuda:12.2.0-devel-ubuntu22.04 nvidia-smi
```

### Problemas de Memoria
```bash
# Aumentar l√≠mite de memoria en docker-compose.yml
mem_limit: 16g

# Verificar uso de memoria
docker stats
```

### Logs y Depuraci√≥n
```bash
# Ver logs completos
docker-compose logs -f ollama

# Verificar configuraci√≥n
docker-compose config

# Ejecutar en modo interactivo
docker-compose run --rm ollama bash
```

## üìÅ Estructura de Archivos

```
.
‚îú‚îÄ‚îÄ Dockerfile              # Imagen de Docker
‚îú‚îÄ‚îÄ docker-compose.yml      # Configuraci√≥n del servicio
‚îú‚îÄ‚îÄ .env                    # Variables de entorno
‚îú‚îÄ‚îÄ entrypoint.sh          # Script de inicio
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ ollama-manager.sh  # Script de gesti√≥n
‚îî‚îÄ‚îÄ README.md              # Este archivo
```

## üîí Seguridad

- El contenedor ejecuta con usuario no-root (`ollama`)
- Los modelos se almacenan en vol√∫menes nombrados para persistencia
- Solo el puerto 11434 est√° expuesto externamente
- La GPU es accesible solo dentro del contenedor

## üìä Monitoreo

### Uso de GPU
```bash
# Ver uso de GPU
watch -n 1 nvidia-smi

# Ver estad√≠sticas
docker stats
```

### Rendimiento
```bash
# Benchmark de modelos
docker-compose exec ollama ollama run deepseek-coder:6.7b "Eval√∫a este c√≥digo:"
```

## üîÑ Actualizaci√≥n

```bash
# Detener servicio
docker-compose down

# Actualizar imagen
docker-compose build --no-cache

# Reiniciar
docker-compose up -d
```

## üìö Recursos Adicionales

- [Documentaci√≥n Ollama](https://ollama.ai)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/)
- [Modelos disponibles](https://ollama.ai/library)
- [API REST](https://ollama.ai/blog/api)

## ü§ù Contribuir

1. Fork el proyecto
2. Crea una rama (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìù Licencia

Este proyecto est√° licenciado bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.